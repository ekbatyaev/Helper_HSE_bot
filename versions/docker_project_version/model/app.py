import os
import torch
import requests
import json
import numpy as np
from pathlib import Path
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Depends
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModel
from torch.nn.functional import cosine_similarity
from typing import Dict, Any
import logging

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π
logging.basicConfig(level=logging.INFO)

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MODEL_NAME = "intfloat/multilingual-e5-large"
API_TOKEN = os.getenv("API_TOKEN")
NOTION_TOKEN = os.getenv("NOTION_TOKEN")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = None
model = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global tokenizer, model

    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    logger.info("Loading tokenizer and model...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        model = AutoModel.from_pretrained(MODEL_NAME)
        logger.info("Model and tokenizer loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load model: {str(e)}")
        raise

    yield

    # –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
    logger.info("Shutting down...")


app = FastAPI(title="HSE Chatbot Model API", lifespan=lifespan)


class SearchRequest(BaseModel):
    question: str
    faculty_name: str

def verify_token(token: str):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞"""
    if token != API_TOKEN:
        logger.warning(f"Unauthorized access attempt with token: {token}")
        raise HTTPException(
            status_code=401,
            detail="Unauthorized",
            headers={"WWW-Authenticate": "Bearer"}
        )
    return token


async def get_pages(page_id: str, headers: Dict[str, str], num_pages: int = None) -> list:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü –∏–∑ Notion API"""
    url = f"https://api.notion.com/v1/databases/{page_id}/query"
    get_all = num_pages is None
    page_size = 100 if get_all else num_pages

    try:
        payload = {"page_size": page_size}
        response = requests.post(url, json=payload, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()

        results = data["results"]
        while data["has_more"] and get_all:
            payload = {"page_size": page_size, "start_cursor": data["next_cursor"]}
            response = requests.post(url, json=payload, headers=headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            results.extend(data["results"])

        return results

    except requests.exceptions.RequestException as e:
        logger.error(f"Notion API request failed: {str(e)}")
        raise HTTPException(
            status_code=503,
            detail="Notion API service unavailable"
        )


async def extract_rich_text(rich_text_array: list) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ Notion"""
    content = ""
    for item in rich_text_array:
        text_content = item.get("text", {}).get("content", "")
        annotations = item.get("annotations", {})

        if annotations.get("bold"):
            text_content = f"*{text_content}*"
        if annotations.get("italic"):
            text_content = f"_{text_content}_"
        if annotations.get("underline"):
            text_content = f"~{text_content}~"

        content += text_content
    return content


async def parse_questions(fac_page_id: str, headers: Dict[str, str], common_questions_page_id: str) -> list:
    """–ü–∞—Ä—Å–∏–Ω–≥ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ Notion"""
    information = []
    all_len = 0

    for page_id in [common_questions_page_id, fac_page_id]:
        try:
            pages = await get_pages(page_id, headers)
            for page in pages:
                props = page["properties"]
                question = "".join([
                    part.get("text", {}).get("content", "")
                    for part in props.get("–í–æ–ø—Ä–æ—Å", {}).get("title", [])
                ])
                answer = props.get("–û—Ç–≤–µ—Ç", {}).get("rich_text", [])
                formatted_answer = await extract_rich_text(answer)
                all_len += len(question) + len(formatted_answer)
                information.append([0, question, formatted_answer])

        except Exception as e:
            logger.error(f"Error parsing questions for page {page_id}: {str(e)}")
            continue

    return [information, all_len]


async def get_embeddings(texts: list) -> torch.Tensor:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
    try:
        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :]

    except Exception as e:
        logger.error(f"Embedding generation failed: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail="Embedding generation error"
        )


async def load_embeddings(faculty_name: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    file_path = Path(__file__).parent / "embeddings"
    file_path.mkdir(exist_ok=True)

    headers = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28",
    }

    try:
        data = await load_data(file_path / "emb_info.json")
        common_questions_page_id = data.get("common_questions", {}).get("page_id")

        information, symbols_count = await parse_questions(
            data[faculty_name]["page_id"], headers, common_questions_page_id
        )

        emb_file = file_path / f"emb_{faculty_name}.npy"

        if data[faculty_name].get("symbols_count") != symbols_count or not emb_file.exists():
            questions = [cell[1] for cell in information]
            questions_embeddings = await get_embeddings(questions)
            data[faculty_name]["symbols_count"] = symbols_count

            np.save(emb_file, questions_embeddings.numpy())
            await save_data(file_path / "emb_info.json", data)
        else:
            questions_embeddings = torch.from_numpy(np.load(emb_file))

        for i, emb in enumerate(questions_embeddings):
            information[i][0] = emb

        return information

    except Exception as e:
        logger.error(f"Error loading embeddings for {faculty_name}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to load embeddings: {str(e)}"
        )


async def load_data(data_file: Path) -> Dict:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        with open(data_file, 'r', encoding='utf-8') as file:
            return json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


async def save_data(data_file: Path, data: Dict) -> None:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(data_file, 'w', encoding='utf-8') as file:
            json.dump(data, file, ensure_ascii=False, indent=4)
    except Exception as e:
        logger.error(f"Failed to save data: {str(e)}")


@app.post("/search")
async def search_api(request: SearchRequest, token: str = Depends(verify_token)):
    """–ü–æ–∏—Å–∫ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã"""
    try:
        logger.info(f"Processing question: {request.question[:50]}... for {request.faculty_name}")

        pre_work_info = await load_embeddings(request.faculty_name)
        if not pre_work_info:
            return {
                "question": request.question,
                "error": "No data available",
                "answer": "–î–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            }

        user_question_emb = await get_embeddings([request.question])
        database_questions_embed = torch.stack([torch.as_tensor(cell[0]) for cell in pre_work_info])

        similarities = cosine_similarity(
            user_question_emb.unsqueeze(1),
            database_questions_embed.unsqueeze(0),
            dim=-1
        )

        if similarities.numel() == 0:
            logger.warning("No similarities found")
            return {
                "question": request.question,
                "error": "No similarities found",
                "answer": "–Ø –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å üôÅ"
            }

        best_match_idx = similarities.argmax().item()
        if best_match_idx >= len(pre_work_info):
            logger.error(f"Index {best_match_idx} out of range for {len(pre_work_info)} items")
            return {
                "question": request.question,
                "error": "Index out of range",
                "answer": "–Ø –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å üôÅ"
            }

        best_match = pre_work_info[best_match_idx][1]
        best_score = similarities.flatten()[best_match_idx].item()
        context = pre_work_info[best_match_idx][2]

        print(f"answer: {request.question} {best_score} {best_match}")

        # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        max_context_length = 512  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        context_parts = [context[i:i + max_context_length] for i in range(0, len(context), max_context_length)]

        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_embs = []
        for part in context_parts:
            emb = await get_embeddings(part)
            context_embs.append(emb)

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –∫–∞–∂–¥–æ–π —á–∞—Å—Ç—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        max_context_similarity = 0
        for emb in context_embs:
            similarities_context = cosine_similarity(
                user_question_emb.unsqueeze(1), emb.unsqueeze(0), dim=-1
            )
            current_sim = similarities_context.item()
            if current_sim > max_context_similarity:
                max_context_similarity = current_sim

        print(f"Max context similarity: {max_context_similarity}")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–∞ —É—Å–ª–æ–≤–∏—è: —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –≤–æ–ø—Ä–æ—Å–æ–º –∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        if best_score < 0.85 or (best_score < 0.89 and max_context_similarity < 0.85):
            return {
                "question": request.question,
                "score": best_score,
                "best_match": best_match,
                "answer": "–Ø –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ç–≤–æ–π –≤–æ–ø—Ä–æ—Å üôÅ"
            }
        return {
            "question": request.question,
            "score": best_score,
            "best_match": best_match,
            "answer": pre_work_info[best_match_idx][2]
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error"
        )


@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    return {
        "status": "ok",
        "model": MODEL_NAME,
        "model_loaded": model is not None,
        "tokenizer_loaded": tokenizer is not None
    }


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_config=None,
        access_log=False,
        lifespan="on"
    )